{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lynn_Kelly_Tchoua_Optimal_Brain_Damage.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQWvQ7s8ZfIz"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SPC3d5hZzHR"
      },
      "source": [
        "import os\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFFe72dRbM-_",
        "outputId": "4490cd23-b140-4ec1-f64f-ec814b906677"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Using {} device'.format(device))"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koLRqCRlbY6D"
      },
      "source": [
        " ##parameters definition\n",
        " \n",
        " input_size = 784 # 28x28\n",
        " hidden_size = 500 \n",
        " num_classes = 10\n",
        " num_epochs = 3\n",
        " batch_size = 100\n",
        " learning_rate = 0.01 "
      ],
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQshAcUz5Ara"
      },
      "source": [
        "# Loading the MNIST dataset \n",
        "train_dataset = datasets.MNIST(root='./data', \n",
        "                                            train=True, \n",
        "                                       transform=transforms.ToTensor(),  \n",
        "                                           download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', \n",
        "                                           train=False, \n",
        "                                           transform=transforms.ToTensor()) "
      ],
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpRjhS025L66"
      },
      "source": [
        "# Importing Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                            batch_size=batch_size, \n",
        "                                            shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=False) "
      ],
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8Xb3CunmodD"
      },
      "source": [
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        \n",
        "        ## cnn layers \n",
        "        self.conv1 = nn.Sequential(         \n",
        "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2),                              \n",
        "            nn.ReLU(),                      \n",
        "            nn.MaxPool2d(kernel_size=2),    \n",
        "        )\n",
        "       \n",
        "        self.conv2 = nn.Sequential(         \n",
        "            nn.Conv2d(16, 32, 5, 1, 2),     \n",
        "            nn.ReLU(),                      \n",
        "            nn.MaxPool2d(2),                \n",
        "        )\n",
        "        # fully connected layer, output 10 classes\n",
        "        self.out = nn.Linear(32 * 7 * 7, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        \n",
        "        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
        "        \n",
        "        x = x.reshape(x.shape[0], -1) \n",
        "\n",
        "        output = self.out(x)\n",
        "        return output, x    # return x for visualization"
      ],
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iYuzqnj5wLH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3761568e-7879-49fc-9370-875281d683b3"
      },
      "source": [
        "#Defining our model \n",
        "\n",
        "model = LeNet().to(device)\n",
        "model"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LeNet(\n",
              "  (conv1): Sequential(\n",
              "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv2): Sequential(\n",
              "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (out): Linear(in_features=1568, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dk6oFOgAAKoh"
      },
      "source": [
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) "
      ],
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cO_Stao4w93d"
      },
      "source": [
        "train_losses = []\n",
        "train_counter = []\n",
        "test_losses = []\n",
        "test_counter = [i*len(train_loader.dataset) for i in range(num_epochs + 1)]"
      ],
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_NxOfZhAltS"
      },
      "source": [
        "  def train(epoch):\n",
        "\n",
        "    model.train()\n",
        "    #n_total_steps = len(train_loader)\n",
        "  \n",
        "    \n",
        "    for i, (images, labels) in enumerate(train_loader):  \n",
        "        # origin shape: [100, 1, 28, 28]\n",
        "        # resized: [100, 784]\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs[0], labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "          print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, i * len(images), len(train_loader.dataset),100. * i / len(train_loader), loss.item()))\n",
        "          train_losses.append(loss.item())\n",
        "          train_counter.append((i*64) + ((epoch-1)*len(train_loader.dataset)))\n",
        "          torch.save(model.state_dict(), '/content/results/model.pth')\n",
        "    \n"
      ],
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpjmDyQUsxMn"
      },
      "source": [
        "def test():\n",
        "\n",
        "  model.eval()\n",
        "  \n",
        "  n_correct = 0\n",
        "  test_loss = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "     \n",
        "     for images, labels in test_loader:\n",
        "         images = images.to(device)\n",
        "         labels = labels.to(device)\n",
        "\n",
        "         outputs = model(images)\n",
        "         test_loss += F.nll_loss(outputs[0], labels, size_average=False).item()\n",
        "         # max returns (value ,index)\n",
        "\n",
        "         predicted = outputs[0].data.max(1, keepdim=True)[1]\n",
        "\n",
        "         n_correct += predicted.eq(labels.data.view_as(predicted)).sum()\n",
        "\n",
        "     test_loss /= len(test_loader.dataset)\n",
        "     test_losses.append(test_loss)\n",
        "     print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, n_correct, len(test_loader.dataset), 100. * n_correct / len(test_loader.dataset)))"
      ],
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXKHjaAJeG2B",
        "outputId": "d72380af-9b42-49df-ac7f-376c63923095"
      },
      "source": [
        "for x, y in train_loader:\n",
        "  # print(x.size())\n",
        "  out = model(x.to(device))\n",
        "  #print(out[1].shape)\n",
        "  print (out[0].data.max(1, keepdim=True)[1].shape)\n",
        "  break"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZh2KQ8uQBAg",
        "outputId": "11b64cc0-d972-49df-ded0-1d28aad10ce6"
      },
      "source": [
        "!mkdir results"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘results’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3D9JVWVuzlDs",
        "outputId": "1235e833-d22b-4262-c99a-e3fd1b335354"
      },
      "source": [
        "for epoch in range(1, num_epochs+1):\n",
        "  train(epoch)\n",
        "  test()"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.301420\n",
            "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 0.099836\n",
            "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.122429\n",
            "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 0.040324\n",
            "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.070022\n",
            "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 0.088839\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Avg. loss: -9.6525, Accuracy: 9829/10000 (98%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.062003\n",
            "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 0.037575\n",
            "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.076708\n",
            "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.138790\n",
            "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.067886\n",
            "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 0.054056\n",
            "\n",
            "Test set: Avg. loss: -10.8578, Accuracy: 9861/10000 (99%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.073194\n",
            "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 0.015196\n",
            "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 0.074773\n",
            "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 0.155476\n",
            "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.019139\n",
            "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 0.152860\n",
            "\n",
            "Test set: Avg. loss: -10.7685, Accuracy: 9825/10000 (98%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjpEyY46uX0J",
        "outputId": "c4cf092e-fee8-4b40-dd71-ea969949faec"
      },
      "source": [
        "loaded_model = LeNet().to(device)\n",
        "loaded_model.load_state_dict(torch.load('/content/results/model.pth'))\n",
        "loaded_model.eval()"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LeNet(\n",
              "  (conv1): Sequential(\n",
              "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv2): Sequential(\n",
              "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (out): Linear(in_features=1568, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 201
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "WXeSNjfXvTEW",
        "outputId": "4b12222b-1426-4d43-9924-1441cf4d9980"
      },
      "source": [
        "for name, param in loaded_model.named_parameters():\n",
        "  \n",
        "  Hessian_params = torch.autograd.functional.hessian(loss, param,)"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-224-3ba10040f1e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mHessian_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhessian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'loss' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "79z4pb_WK7J6",
        "outputId": "253bdf75-7f5f-44b2-8290-167c39e79c4a"
      },
      "source": [
        "def my_loss(outputs, labels):\n",
        "    loss = criterion(outputs, labels)\n",
        "    return loss\n",
        "\n",
        "for i, (images, labels) in enumerate(train_loader):  \n",
        "      \n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "      # Forward pass\n",
        "      outputs = loaded_model(images)\n",
        "\n",
        "Hess_params = torch.autograd.functional.hessian(my_loss, (outputs, labels))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f4f160741848>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xi4V35QtXDl5"
      },
      "source": [
        " def compute_hessian():\n",
        "\n",
        "  for i, (images, labels) in enumerate(train_loader):  \n",
        "      \n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "      # Forward pass\n",
        "      outputs = loaded_model(images)\n",
        "     \n",
        "\n",
        "      for name, param in loaded_model.named_parameters():\n",
        "        \n",
        "        p = param\n",
        "        loss = criterion(outputs[0], labels)\n",
        "        print(loss)\n",
        "       \n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "      \n",
        "        loss.backward(retain_graph=True)\n",
        "        \n",
        "        grad_params = torch.autograd.grad(loss, p, create_graph=True,allow_unused=True)  # p is the weight matrix for a particular layer \n",
        "        \n",
        "        hess_params = torch.zeros_like(grad_params[0])\n",
        "        \n",
        "\n",
        "        for i in range(grad_params[0].size(0)):\n",
        "            for j in range(grad_params[0].size(1)):\n",
        "              \n",
        "              hess_params[i, j] = torch.autograd.grad(grad_params[0][i][j], p, retain_graph=True)[0][i, j]\n",
        "              \n",
        "        optimizer.step()\n",
        "\n",
        "        return hess_params\n"
      ],
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cy4gT31Of6zK"
      },
      "source": [
        "h=compute_hessian()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avEd3AcU5pOe"
      },
      "source": [
        "def train_OBD(epoch):\n",
        "\n",
        "  model.train()\n",
        "  n_total_steps = len(train_loader)\n",
        "  \n",
        "    \n",
        "  for i, (images, labels) in enumerate(train_loader):  \n",
        "      # origin shape: [100, 1, 28, 28]\n",
        "      # resized: [100, 784]\n",
        "      images = images.reshape(-1, 28*28).to(device)\n",
        "      labels = labels.to(device)\n",
        "      # Forward pass\n",
        "      outputs = model(images)\n",
        "\n",
        "      for name, param in model.named_parameters():\n",
        "        \n",
        "        p = param\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "      \n",
        "        loss.backward(retain_graph=True)\n",
        "        grad_params = torch.autograd.grad(loss, p, create_graph=True)  # p is the weight matrix for a particular layer \n",
        "        hess_params = torch.zeros_like(grad_params[0])\n",
        "\n",
        "        for i in range(grad_params[0].size(0)):\n",
        "            for j in range(grad_params[0].size(1)):\n",
        "                hess_params[i, j] = torch.autograd.grad(grad_params[0][i][j], p, retain_graph=True)[0][i, j]\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "          print('Train_OBD Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "            epoch, i * len(images), len(train_loader.dataset),\n",
        "            100. * i / len(train_loader), loss.item()))\n",
        "          train_losses.append(loss.item())\n",
        "          train_counter.append((i*64) + ((epoch-1)*len(train_loader.dataset)))\n",
        "\n",
        "\n",
        "  return grad_params, hess_params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "id": "N1RxLvMYTQwV",
        "outputId": "1ce3ce7f-cdd6-43d1-f1a6-62fd2e36c01e"
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "  train_OBD(epoch)\n",
        "  test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/3], Step[500/600], Loss: 0.0179\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-1b3875c87b80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mtrain_OBD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-50e0f99efba1>\u001b[0m in \u001b[0;36mtrain_OBD\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mgrad_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# p is the weight matrix for a particular layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mhess_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [500, 10]], which is output 0 of TBackward, is at version 3602; expected version 3601 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dghvWrQMWFg_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}